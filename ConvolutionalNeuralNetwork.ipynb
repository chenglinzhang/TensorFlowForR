{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem: Given a list of handwritten digit images and their labels, train a model to recognize digits \n",
    "    with a convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tensorflow for R library\n",
    "library(tensorflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fetch data\n",
    "datasets <- tf$contrib$learn$datasets\n",
    "mnist <- datasets$mnist$read_data_sets(\"MNIST-data\", one_hot = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input place holder\n",
    "x <- tf$placeholder(tf$float32, shape(NULL, 28L * 28L))\n",
    "x_image <- tf$reshape(x, shape(-1L, 28L, 28L, 1L))\n",
    "# output place holder\n",
    "y_ <- tf$placeholder(tf$float32, shape(NULL, 10L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utility function for weight variable\n",
    "weight_variable <- function(shape) {\n",
    "    initial <- tf$truncated_normal(shape, stddev=0.1)\n",
    "    tf$Variable(initial)\n",
    "}\n",
    "\n",
    "# utility function for bias variable\n",
    "bias_variable <- function(shape) {\n",
    "    initial <- tf$constant(0.1, shape=shape)\n",
    "    tf$Variable(initial)\n",
    "}\n",
    "\n",
    "# utility function for convolution 2D\n",
    "conv2d <- function(x, W) {\n",
    "    tf$nn$conv2d(x, W, strides=c(1L, 1L, 1L, 1L), padding='SAME')\n",
    "}\n",
    "\n",
    "# utility function for max pooling\n",
    "max_pool_2x2 <- function(x) {\n",
    "    tf$nn$max_pool(x, ksize=c(1L, 2L, 2L, 1L), strides=c(1L, 2L, 2L, 1L), padding='SAME')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convolution model\n",
    "\n",
    "# first layer 5x5 with 32 filters\n",
    "W_conv1 <- weight_variable(shape(5L, 5L, 1L, 32L))\n",
    "b_conv1 <- bias_variable(shape(32L))\n",
    "h_conv1 <- tf$nn$relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 <- max_pool_2x2(h_conv1)\n",
    "\n",
    "# second layer 5x5 with 64 features\n",
    "W_conv2 <- weight_variable(shape = shape(5L, 5L, 32L, 64L))\n",
    "b_conv2 <- bias_variable(shape = shape(64L))\n",
    "h_conv2 <- tf$nn$relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 <- max_pool_2x2(h_conv2)\n",
    "\n",
    "# image size reduced to 7x7, add a fully connected layer with 1024 neurons\n",
    "W_fc1 <- weight_variable(shape(7L * 7L * 64L, 1024L))\n",
    "b_fc1 <- bias_variable(shape(1024L))\n",
    "h_pool2_flat <- tf$reshape(h_pool2, shape(-1L, 7L * 7L * 64L))\n",
    "h_fc1 <- tf$nn$relu(tf$matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "# dropout layer\n",
    "keep_prob <- tf$placeholder(tf$float32)\n",
    "h_fc1_drop <- tf$nn$dropout(h_fc1, keep_prob)\n",
    "\n",
    "# softmax layer\n",
    "W_fc2 <- weight_variable(shape(1024L, 10L))\n",
    "b_fc2 <- bias_variable(shape(10L))\n",
    "y_conv <- tf$nn$softmax(tf$matmul(h_fc1_drop, W_fc2) + b_fc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 20, training accuracy 0.5\n",
      "step 40, training accuracy 0.78\n",
      "step 60, training accuracy 0.8\n",
      "step 80, training accuracy 0.76\n",
      "step 100, training accuracy 0.92\n",
      "step 120, training accuracy 0.92\n",
      "step 140, training accuracy 0.92\n",
      "step 160, training accuracy 0.84\n",
      "step 180, training accuracy 0.88\n",
      "step 200, training accuracy 0.92\n",
      "step 220, training accuracy 0.86\n",
      "step 240, training accuracy 0.88\n",
      "step 260, training accuracy 0.94\n",
      "step 280, training accuracy 0.9\n",
      "step 300, training accuracy 0.96\n",
      "step 320, training accuracy 0.88\n",
      "step 340, training accuracy 0.94\n",
      "step 360, training accuracy 0.98\n",
      "step 380, training accuracy 0.96\n",
      "step 400, training accuracy 0.92\n",
      "step 420, training accuracy 1\n",
      "step 440, training accuracy 1\n",
      "step 460, training accuracy 0.92\n",
      "step 480, training accuracy 0.94\n",
      "step 500, training accuracy 0.88\n",
      "step 520, training accuracy 0.94\n",
      "step 540, training accuracy 0.92\n",
      "step 560, training accuracy 0.98\n",
      "step 580, training accuracy 0.96\n",
      "step 600, training accuracy 0.98\n",
      "step 620, training accuracy 0.94\n",
      "step 640, training accuracy 0.98\n",
      "step 660, training accuracy 0.98\n",
      "step 680, training accuracy 0.92\n",
      "step 700, training accuracy 0.94\n",
      "step 720, training accuracy 0.92\n",
      "step 740, training accuracy 0.94\n",
      "step 760, training accuracy 0.9\n",
      "step 780, training accuracy 0.94\n",
      "step 800, training accuracy 0.92\n",
      "step 820, training accuracy 0.96\n",
      "step 840, training accuracy 0.96\n",
      "step 860, training accuracy 0.96\n",
      "step 880, training accuracy 0.94\n",
      "step 900, training accuracy 0.94\n",
      "step 920, training accuracy 1\n",
      "step 940, training accuracy 0.92\n",
      "step 960, training accuracy 0.9\n",
      "step 980, training accuracy 0.94\n",
      "step 1000, training accuracy 0.96\n",
      "step 1020, training accuracy 0.98\n",
      "step 1040, training accuracy 1\n",
      "step 1060, training accuracy 0.98\n",
      "step 1080, training accuracy 1\n",
      "step 1100, training accuracy 1\n",
      "step 1120, training accuracy 0.98\n",
      "step 1140, training accuracy 0.96\n",
      "step 1160, training accuracy 1\n",
      "step 1180, training accuracy 0.96\n",
      "step 1200, training accuracy 1\n",
      "step 1220, training accuracy 0.98\n",
      "step 1240, training accuracy 0.9\n",
      "step 1260, training accuracy 0.96\n",
      "step 1280, training accuracy 0.98\n",
      "step 1300, training accuracy 0.96\n",
      "step 1320, training accuracy 0.96\n",
      "step 1340, training accuracy 0.98\n",
      "step 1360, training accuracy 0.98\n",
      "step 1380, training accuracy 0.98\n",
      "step 1400, training accuracy 0.98\n",
      "step 1420, training accuracy 0.96\n",
      "step 1440, training accuracy 0.96\n",
      "step 1460, training accuracy 0.94\n",
      "step 1480, training accuracy 0.96\n",
      "step 1500, training accuracy 1\n",
      "step 1520, training accuracy 0.98\n",
      "step 1540, training accuracy 0.96\n",
      "step 1560, training accuracy 0.96\n",
      "step 1580, training accuracy 0.94\n",
      "step 1600, training accuracy 0.96\n",
      "step 1620, training accuracy 0.98\n",
      "step 1640, training accuracy 0.96\n",
      "step 1660, training accuracy 0.98\n",
      "step 1680, training accuracy 0.98\n",
      "step 1700, training accuracy 1\n",
      "step 1720, training accuracy 0.98\n",
      "step 1740, training accuracy 1\n",
      "step 1760, training accuracy 0.98\n",
      "step 1780, training accuracy 0.98\n",
      "step 1800, training accuracy 0.98\n",
      "step 1820, training accuracy 0.98\n",
      "step 1840, training accuracy 0.92\n",
      "step 1860, training accuracy 0.98\n",
      "step 1880, training accuracy 0.98\n",
      "step 1900, training accuracy 1\n",
      "step 1920, training accuracy 1\n",
      "step 1940, training accuracy 1\n",
      "step 1960, training accuracy 1\n",
      "step 1980, training accuracy 0.98\n",
      "step 2000, training accuracy 0.96\n"
     ]
    }
   ],
   "source": [
    "# entropy and optimizer\n",
    "cross_entropy <- tf$reduce_mean(-tf$reduce_sum(y_ * tf$log(y_conv), reduction_indices=1L))\n",
    "train_step <- tf$train$AdamOptimizer(1e-4)$minimize(cross_entropy)\n",
    "\n",
    "# accuracy\n",
    "correct_prediction <- tf$equal(tf$argmax(y_conv, 1L), tf$argmax(y_, 1L))\n",
    "accuracy <- tf$reduce_mean(tf$cast(correct_prediction, tf$float32))\n",
    "\n",
    "# session\n",
    "sess = tf$Session()\n",
    "sess$run(tf$global_variables_initializer())\n",
    "\n",
    "# training loop\n",
    "for (i in 1:2000) {\n",
    "    # mini batch\n",
    "    batch <- mnist$train$next_batch(50L)\n",
    "    if (i %% 20 == 0) {\n",
    "        train_accuracy <- sess$run(accuracy, feed_dict = dict(x = batch[[1]], y_ = batch[[2]], keep_prob = 1.0))\n",
    "        cat(sprintf(\"step %d, training accuracy %g\\n\", i, train_accuracy))\n",
    "    }    \n",
    "    sess$run(train_step, feed_dict = dict(x = batch[[1]], y_ = batch[[2]], keep_prob = 0.5))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 0.9748"
     ]
    }
   ],
   "source": [
    "# test accuracy\n",
    "test_accuracy <- sess$run(accuracy, feed_dict = dict(x = mnist$test$images, y_ = mnist$test$labels, keep_prob = 1.0))\n",
    "cat(sprintf(\"test accuracy %g\", test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
